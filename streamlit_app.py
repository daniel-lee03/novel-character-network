# -*- coding: utf-8 -*-
# ì‹¤í–‰: streamlit run streamlit_app.py --server.port 8501 --server.address 0.0.0.0
import os, re, json, time, itertools
from typing import List, Set, Dict, Tuple
import streamlit as st
import pandas as pd
import networkx as nx
from pyvis.network import Network
import kss
import streamlit.components.v1 as components
import requests

# =============== ê¸°ë³¸ UI ===============
st.set_page_config(page_title="ì†Œì„¤ ì¸ë¬¼ ê´€ê³„ ë„¤íŠ¸ì›Œí¬(ì´ˆê°„ë‹¨)", page_icon="ğŸ“š", layout="wide")
st.title("ğŸ“š ì†Œì„¤ ì¸ë¬¼ ê´€ê³„ ë„¤íŠ¸ì›Œí¬ â€” ì°¨ë¯¸ / ì˜¤ë€ / ë…¹ì£¼")

with st.sidebar:
    st.header("ì˜µì…˜")
    st.caption("ë“±ì¥ ì¸ë¬¼ì€ ê³ ì •: ì°¨ë¯¸, ì˜¤ë€, ë…¹ì£¼")
    window = st.slider("ë™ì‹œì¶œí˜„ ìœˆë„ìš°(ë¬¸ì¥ ë‹¨ìœ„)", 1, 3, 1)
    min_edge = st.slider("ì—£ì§€ ê°€ì¤‘ì¹˜ ì„ê³„ê°’", 1, 5, 1)
    use_hf = st.toggle("Hugging Face ê°ì • ë¶„ì„ ì‚¬ìš©(ì„ íƒ)", value=False,
                       help="ë¬¸ì¥ ê°ì •(ê¸/ë¶€ì •)ì„ í‰ê·  ë‚´ì„œ ì—£ì§€ ìƒ‰ì— ë°˜ì˜í•©ë‹ˆë‹¤.")
    hf_model = st.text_input("ê°ì • ëª¨ë¸ ID", value="nlpai-lab/klue-roberta-base-finetuned-nsmc",
                             help="í•œêµ­ì–´ ê°ì •(ê¸/ë¶€ì •) ë¶„ë¥˜ ëª¨ë¸ ì˜ˆì‹œ")
    hf_token = st.text_input("HF_TOKEN", type="password",
                             help="í† í°ì´ ì—†ìœ¼ë©´ ê³µê°œëª¨ë¸ì—ì„œ ëŠë¦´ ìˆ˜ ìˆì–´ìš”(ì„ íƒ).")

SAMPLE = """ì˜¤ëŠ˜ì„ ì–¼ë§ˆë‚˜ ê¸°ë‹¤ë ¸ëŠ”ì§€ ëª¨ë¥¸ë‹¤. ì°¨ë¯¸ëŠ” ê°€ë°©ì„ ì—¬ë¯¸ê³  ì˜¤ë€ì„ ê¸°ë‹¤ë ¸ë‹¤.
ì˜¤ë€ì€ ëŠ¦ê²Œ ë„ì°©í–ˆê³ , ë…¹ì£¼ëŠ” ë‘ ì‚¬ëŒì„ ë©€ë¦¬ì„œ ë°”ë¼ë³´ì•˜ë‹¤.
ì°¨ë¯¸ì™€ ë…¹ì£¼ëŠ” ì§§ê²Œ ì¸ì‚¬ë¥¼ ë‚˜ëˆ„ê³ , ì˜¤ë€ì€ ë¯¸ì•ˆí•˜ë‹¤ê³  ë§í–ˆë‹¤.
ê·¸ë‚  ì´í›„ ì°¨ë¯¸ì™€ ì˜¤ë€, ë…¹ì£¼ì˜ ê´€ê³„ëŠ” ì¡°ê¸ˆì”© ë‹¬ë¼ì¡Œë‹¤."""
text = st.text_area("ì†Œì„¤ ë³¸ë¬¸ ë¶™ì—¬ë„£ê¸°", value=SAMPLE, height=220)
uploaded = st.file_uploader("ë˜ëŠ” .txt ì—…ë¡œë“œ", type=["txt"])
if uploaded:
    text = uploaded.read().decode("utf-8", errors="ignore")

run = st.button("ë¶„ì„ ì‹¤í–‰")

# =============== ìœ í‹¸ ===============
NAMES = ["ì°¨ë¯¸", "ì˜¤ë€", "ë…¹ì£¼"]

def normalize_name(s: str) -> str:
    s = s.strip()
    # í˜¸ì¹­/ì ‘ë¯¸ì‚¬ ê°„ë‹¨ ì œê±°
    s = re.sub(r"(ì”¨|ë‹˜|êµ°|ì–‘|ì„ ìƒë‹˜?)$", "", s)
    return s

# â€˜ì°¨ë¯¸/ì˜¤ë€/ë…¹ì£¼â€™ ë‹¤ì–‘í•œ í‘œê¸° í—ˆìš©(í˜¸ì¹­ í¬í•¨)
NAME_RX = re.compile(r"\b(ì°¨ë¯¸|ì˜¤ë€|ë…¹ì£¼)(ì”¨|ë‹˜|êµ°|ì–‘|ì„ ìƒë‹˜?)?\b")

def split_sentences(t: str) -> List[str]:
    # ë§¤ìš° ì•ˆì •ì ì¸ í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ê¸°
    return [s.strip() for s in kss.split_sentences(t) if s.strip()]

def sentence_window_indices(i: int, w: int, n: int) -> List[int]:
    if w <= 1: return [i]
    idxs = {i}
    for k in range(1, w):
        if i-k >= 0: idxs.add(i-k)
        if i+k < n: idxs.add(i+k)
    return sorted(idxs)

# ---- Hugging Face Inference API (ì„ íƒ) ----
def hf_sentiment(sentence: str, model_id: str, token: str = "", timeout: float = 15.0) -> float:
    """
    ë°˜í™˜: [+1.0 ~ -1.0] ë²”ìœ„ì˜ ìŠ¤ì½”ì–´. (ê¸ì •â‰ˆ+1, ë¶€ì •â‰ˆ-1)
    """
    url = f"https://api-inference.huggingface.co/models/{model_id}"
    headers = {"Authorization": f"Bearer {token}"} if token else {}
    payload = {"inputs": sentence}
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=timeout)
        if r.status_code == 503:
            # ëª¨ë¸ ë¡œë“œ ëŒ€ê¸°
            time.sleep(2.0)
            r = requests.post(url, headers=headers, json=payload, timeout=timeout)
        r.raise_for_status()
        out = r.json()
        # ì˜ˆ: [{"label":"positive","score":0.98},{"label":"negative","score":0.02}]
        if isinstance(out, list) and out and isinstance(out[0], dict):
            scores = {d.get("label", "").lower(): d.get("score", 0.0) for d in out}
            pos = scores.get("positive", 0.0)
            neg = scores.get("negative", 0.0)
            return float(pos - neg)  # +ë©´ ê¸ì •, -ë©´ ë¶€ì •
    except Exception:
        pass
    return 0.0  # ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½

# =============== ì‹¤í–‰ ë¡œì§ ===============
if run:
    if not text.strip():
        st.warning("ë³¸ë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    sentences = split_sentences(text)
    st.write(f"ë¬¸ì¥ ìˆ˜: {len(sentences)}")

    # ë¬¸ì¥ë³„ ë“±ì¥ ì¸ë¬¼ ì„¸íŠ¸ + (ì„ íƒ) ë¬¸ì¥ ê°ì •
    per_by_sent: List[Set[str]] = []
    sent_sentiment: List[float] = []

    for s in sentences:
        found = set(normalize_name(m.group(1)) for m in NAME_RX.finditer(s))
        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì ìš©(ê³ ì • 3ì¸)
        found = {n for n in found if n in NAMES}
        per_by_sent.append(found)

        if use_hf:
            sent_sentiment.append(hf_sentiment(s, hf_model, hf_token))
        else:
            sent_sentiment.append(0.0)

    # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ êµ¬ì„±
    G = nx.Graph()
    freq: Dict[str, int] = {n: 0 for n in NAMES}
    pair_sent_scores: Dict[Tuple[str, str], list] = {}

    n = len(per_by_sent)
    for i in range(n):
        idxs = sentence_window_indices(i, window, n)
        people = set().union(*[per_by_sent[j] for j in idxs])
        for p in people:
            freq[p] += 1
            if p not in G:
                G.add_node(p)
        for a, b in itertools.combinations(sorted(list(people)), 2):
            if G.has_edge(a, b):
                G[a][b]["weight"] += 1
            else:
                G.add_edge(a, b, weight=1)
            # ê°ì • ìŠ¤ì½”ì–´ ìˆ˜ì§‘(ì„ íƒ)
            if use_hf:
                key = (a, b)
                pair_sent_scores.setdefault(key, []).append(sent_sentiment[i])

    # ì—£ì§€ ì„ê³„ê°’ í•„í„°
    to_remove = [(u, v) for u, v, d in G.edges(data=True) if d.get("weight", 0) < min_edge]
    G.remove_edges_from(to_remove)

    # ìš”ì•½ í‘œ
    nodes_df = pd.DataFrame(
        [{"name": n, "degree": G.degree(n) if n in G else 0, "frequency": freq.get(n, 0)} for n in NAMES]
    ).sort_values(["degree", "frequency"], ascending=[False, False])

    # --- ì—£ì§€ ìš”ì•½ (ì•ˆì „ íŒ¨ì¹˜) ---
edges_data = [
    {"source": u, "target": v, "weight": d.get("weight", 0)}
    for u, v, d in G.edges(data=True)
]
edges_df = pd.DataFrame(edges_data)

if not edges_df.empty:
    edges_df = edges_df.sort_values("weight", ascending=False)


    col1, col2 = st.columns(2)
    with col1:
        st.subheader("ë…¸ë“œ ìš”ì•½")
        st.dataframe(nodes_df, use_container_width=True, height=260)
        st.download_button("ë…¸ë“œ CSV ë‹¤ìš´ë¡œë“œ", nodes_df.to_csv(index=False).encode("utf-8"),
                           file_name="nodes.csv", mime="text/csv")
    with col2:
        st.subheader("ì—£ì§€ ìš”ì•½")
        st.dataframe(edges_df, use_container_width=True, height=260)
        st.download_button("ì—£ì§€ CSV ë‹¤ìš´ë¡œë“œ", edges_df.to_csv(index=False).encode("utf-8"),
                           file_name="edges.csv", mime="text/csv")

    # Pyvis ì‹œê°í™”
    st.subheader("ê´€ê³„ ë„¤íŠ¸ì›Œí¬(ë“œë˜ê·¸/ì¤Œ ê°€ëŠ¥)")
    net = Network(height="620px", width="100%", bgcolor="#ffffff", font_color="#222222")
    net.barnes_hut(gravity=-20000, central_gravity=0.3, spring_length=160, spring_strength=0.005, damping=0.6)

    # ë…¸ë“œ í‘œì‹œ(ë¹ˆë„ ê¸°ì¤€ í¬ê¸°)
    fmax = max(1, max(freq.values()))
    for n in NAMES:
        if n in G.nodes:
            size = 20 + 30 * (freq[n] / fmax)
            label = f"{n} (deg={G.degree(n)}, f={freq[n]})"
            net.add_node(n, label=label, title=label, size=float(size))
        else:
            # ê·¸ë˜í”„ì— ì•ˆ ëœ¨ë©´ ìŠ¤í‚µ
            pass

    # ì—£ì§€ í‘œì‹œ(ê°€ì¤‘ì¹˜ ë‘ê»˜ + ì„ íƒ ì‹œ ê°ì •ìƒ‰)
    def color_from_score(s: float) -> str:
        # ë¶€ì •ìª½ì€ í‘¸ë¥¸ìƒ‰, ê¸ì •ì€ ë¶‰ì€ìƒ‰ ê³„ì—´(ê°„ë‹¨)
        if s > 0.1: return "#c0392b"   # red-ish
        if s < -0.1: return "#2980b9"  # blue-ish
        return "#7f8c8d"               # neutral gray

    for u, v, d in G.edges(data=True):
        val = int(d.get("weight", 1))
        col = "#7f8c8d"
        title = f"{u}â€“{v} (w={val})"
        if use_hf:
            key = tuple(sorted([u, v]))
            scores = pair_sent_scores.get(key, [])
            if scores:
                avg = sum(scores)/len(scores)
                col = color_from_score(avg)
                title += f" | ê°ì • í‰ê· ={avg:+.2f}"
        net.add_edge(u, v, value=val, title=title, color=col, width=1+val)

    html_path = "graph.html"
    net.show(html_path)
    with open(html_path, "r", encoding="utf-8") as f:
        html = f.read()
    components.html(html, height=640, scrolling=True)

    st.success("ì™„ë£Œ!")

st.markdown("---")
st.caption("ê³ ì • ì¸ë¬¼: ì°¨ë¯¸ Â· ì˜¤ë€ Â· ë…¹ì£¼ | ë¬¸ì¥ ë™ì‹œì¶œí˜„ ê¸°ë°˜ | (ì„ íƒ) Hugging Face ê°ì • ë¶„ë¥˜")
